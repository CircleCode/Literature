<?xml version="1.0" encoding="utf-8"?>
<?xyl-use href="../Definitions.xyl"?>

<definition xmlns="http://hoa-project.net/xyl/xylophone">
<yield name="chapter">

  <h1 id="Core"><em lang="en">Hack book</em><title_break /><code>Compiler</code></h1>

  <p>Les <strong>compilateurs</strong> permettent d'<strong>analyser</strong> et
  <strong>manipuler</strong> des données <strong>textuelles</strong>.  Leurs
  applications sont très nombreuses. <code>Hoa\Compiler</code> propose de
  manipuler plusieurs compilateurs selon les besoins.</p>

  <h2 id="Table_des_matieres" for="menu-toc">Table des matières</h2>

  <tableofcontents id="main-toc" />

  <h2 id="Introduction" for="main-toc menu-toc">Introduction</h2>

  <blockquote cite="https://secure.wikimedia.org/wikipedia/fr/wiki/Nicolas_Boileau">Ce
  qui se conçoit bien s'énonce clairement, et les mots pour le dire viennent
  aisément.</blockquote>
  <p>Un <strong>langage</strong> est une façon d'exprimer ou de
  <strong>formuler</strong> une <strong>solution</strong> à un
  <strong>problème</strong>. Et des problèmes, il en existe beaucoup. Nous
  lisons et écrivons dans plusieurs langages au quotidien, et certains de ces
  langages sont <strong>interprétés</strong> par des <strong>machines</strong>.
  Cette opération est possible grâce aux <strong>compilateurs</strong>.</p>
  <p>La
  <a href="https://secure.wikimedia.org/wikipedia/fr/wiki/Théorie_des_langages">théorie
  des langages</a> étudie entre autre l'<strong>analyse automatique</strong> de
  ses langages à travers des outils comme des <strong>automates</strong> ou des
  <strong>grammaires</strong>. Il est nécessaire d'avoir un cours détaillé pour
  bien comprendre tous ces concepts.  Toutefois, nous allons essayer de
  <strong>vulgariser</strong> un minimum pour permettre une utilisation correcte
  de <code>Hoa\Compiler</code>.</p>

  <h3 id="Langage_et_grammaire" for="main-toc">Langage et grammaire</h3>

  <p>Un <strong>langage</strong> est un ensemble de <strong>mots</strong>.
  Chaque mot est une <strong>séquence</strong> de <strong>symboles</strong>
  appartenant à un <strong>alphabet</strong>. Un symbole représente la plus
  petite <strong>unité lexicale</strong> d'un langage, il est atomique et nous
  l'appellons <strong>lexème</strong> (ou <em lang="en">tokens</em> en anglais).
  Les séquences de lexèmes représentant les mots sont construites avec des
  <strong>règles</strong>. À partir d'une règle racine, nous pouvons
  <strong>dériver</strong> ses sous-règles, jusqu'à construire un mot. Selon la
  <strong>dérivation</strong> que nous faisons des règles, nous allons obtenir
  différents mots. Par exemple, si nous considérons les règles suivantes :</p>
  <pre><code>    exp ::= exp + exp
          | nombre
 nombre ::= chiffre nombre
          | chiffre
chiffre ::= 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9</code></pre>
  <p>La règle racine est <code><em>exp</em></code>. Si nous la dérivons (de
  gauche ) droite et de haut en bas, ou <em lang="en">left-to-right</em> et
  <em lang="en">top-to-bottom</em> en anglais), nous pouvons avoir
  <code><em>exp</em> + <em>exp</em></code> ou <code><em>nombre</em></code> (la
  <strong>disjonction</strong>, <ie /> le « ou », est représenté par le symbole
  « <code>|</code> ») :</p>
  <pre><code>exp + exp | nombre
→ exp + exp
→ ( exp + exp | nombre ) + exp
→ nombre + exp
→ ( chiffre nombre | chiffre ) + exp</code></pre>
  <p>Nous continuons à dériver jusqu'à <strong>éliminer</strong> toutes les
  règles et n'avoir que des <strong>lexèmes</strong> :</p>
  <pre><code>…
→ ( chiffre nombre | chiffre ) + exp
→ chiffre + exp
→ ( 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 ) + exp
→ 7 + exp
→ 7 + ( exp + exp | nombre )
→ 7 + nombre
→ 7 + ( chiffre nombre | chiffre )
→ 7 + chiffre nombre
→ 7 + ( 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 ) nombre
→ 7 + 3 nombre
→ 7 + 3 ( chiffre nombre | chiffre )
→ 7 + 3 chiffre
→ 7 + 3 ( 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 )
→ 7 + 3 5</code></pre>
  <p>Avec cette dérivation, nous obtenons le mot <code>7 + 35</code>.</p>
  <p>Un ensemble de règle est appelé une <strong>grammaire</strong>. Et donc,
  une grammaire représente un <strong>langage</strong> !</p>
  <p>Toutefois, nous nous apercevons qu'il existe <strong>plusieurs</strong>
  façon de dériver les règles. En effet, nous avons choisi
  <strong>arbitrairement</strong> une règle lorsqu'une disjonction était
  rencontrée. Si nous voulons vérifier qu'un mot appartient à un langage, cela
  revient à <strong>trouver</strong> une dérivation permettant de construire un
  <strong>mot équivalent</strong>.</p>
  <p>C'est en 1956 qu'a été formulée la
  <a href="https://secure.wikimedia.org/wikipedia/fr/wiki/Hiérarchie_de_Chomsky">hiérarchie
  de Chomsky</a>, classant les grammaires en quatre <strong>niveaux</strong> :</p>
  <ol start="0">
    <li>grammaires <strong>générales</strong>, ou <em lang="en">unrestricted
    grammars</em>, reconnaissants les langages dits de Turing, aucune
    restriction n'est imposée aux règles ;</li>
    <li>grammaires <strong>contextuelles</strong>, ou
    <em lang="en">context-sensitive grammars</em>, reconnaisants les langages
    contextuels ;</li>
    <li>grammaires <strong>algébriques</strong>, ou <em lang="en">context-free
    grammars</em>, reconnaissants les langages algébriques, basés sur automates
    à pile ;</li>
    <li>grammaires <strong>régulières</strong>, ou <em lang="en">regular
    grammars</em>, reconnaissants les langages réguliers.</li>
  </ol>
  <p>Chaque niveau reconnait le niveau suivant. <code>Hoa\Compiler</code> ne
  traite que les langages définis par les grammaires de niveau 2 et 3. Pour
  donner rapidement une idée, les grammaires régulières peuvent s'apparenter aux
  <a href="https://secure.wikimedia.org/wikipedia/fr/wiki/Expression_régulière">expressions
  régulières</a> (comme les <a href="http://pcre.org/">PCRE</a>), bien connues
  des développeurs. Mais les grammaires régulières ne permettent pas par exemple
  de reconnaître des <strong>couples de symboles</strong> (comme des
  parenthèses, des accolades ou des guillemets), alors que les grammaires
  algébriques le permettent (grâce à la notion de piles de lexèmes).</p>

  <h3 id="Reconnaissance_de_mots" for="main-toc">Reconnaissance de mots</h3>

  <p>En général, le processus de compilation débute par deux
  <strong>analyses</strong> : <strong>lexicale</strong> et
  <strong>syntaxique</strong>. Une analyse lexicale consiste à
  <strong>découper</strong> un mot en une <strong>séquence de lexèmes</strong>.
  Cette séquence sera ensuite utilisée par l'analyseur syntaxique afin de
  vérifier que le mot <strong>appartient</strong> au langage.</p>
  <p>Selon la grammaire, la reconnaissance ne sera fera pas de la même manière,
  mais le principe reste le même : prendre les lexèmes les uns après les autres
  dans la séquence et vérifier qu'ils permettent d'<strong>avancer</strong> dans
  la <strong>dérivation</strong> des règles de notre grammaire.</p>
  <p>Les analyses syntaxiques sont aussi classées en
  <strong>catégories</strong> : LL, LR, LALR etc. <code>Hoa\Compiler</code> ne
  propose que des analyseurs syntaxiques LL, pour <em lang="en">Left-to-right
  Leftmost derivation</em>, <ie /> de la plus haute règle vers la plus profonde,
  et les règles sont dérivées de la gauche vers la droite. Là encore, il existe
  des sous-catégories, dont deux que traite <code>Hoa\Compiler</code> : LL(1)
  et LL(*). D'une manière générale, on parle d'analyseurs syntaxiques
  LL(<em>k</em>) : si un lexème ne permet pas de dériver une règle comme il
  faut, alors l'analyseur peut <strong>revenir</strong> jusqu'à <em>k</em>
  étapes en arrière ; nous parlons aussi de <em lang="en">backtrack</em>.
  Autrement dit, les règles peuvent être <strong>non-déterministes</strong> : à
  chaque fois que nous dérivons une règle de la grammaire, nous avons plusieurs
  choix possibles et l'analyseur peut se tromper, c'est pourquoi il doit parfois
  revenir en arrière. La variable <em>k</em> permet de définir le
  <strong>niveau</strong> de non-déterminisme. Si une grammaire peut être
  analysée par un analyseur syntaxique LL(1), elle est dite
  <strong>déterministe</strong> : à chaque lexème utilisé pour dériver nos
  règles, il n'y a qu'un seul choix possible. Et si nous avons un analyseur
  syntaxique LL(*), cela signifie que la variable <em>k</em> est
  <strong>indéfinie</strong>. L'exemple suivant illustre une grammaire
  déterministe :</p>
  <pre><code></code></pre>
  <p>Et cet exemple illustre une grammaire non-déterministe :</p>
  <pre><code></code></pre>

  <h2 id="Compilateur_de_compilateur_LLk" for="main-toc menu-toc">Compilateur de
  compilateur LL(k)</h2>

  <p>Écrire des compilateurs est une tâche laborieuse. Ce n'est pas forcément
  toujours difficile mais souvent répétitif et long. C'est pourquoi il existe
  des compilateurs de compilateurs, ou autrement dit, des générateurs de
  compilateurs. La plupart du temps, ces compilateurs de compilateurs
  utilisent un langage intermédiaire pour écrire une grammaire, comme un langage
  de programmation d'une certaine manière.</p>
  <p>La bibliothèque <code>Hoa\Compiler</code> propose la classe
  <code>Hoa\Compiler\Llk</code> qui permet l'écriture de compilateurs de
  compilateurs à travers un langage dédié.</p>

  <h3 id="Langage_PP" for="main-toc">Langage PP</h3>

  <p>Le langage PP, pour <em lang="en">PHP Parser</em>, permet d'exprimer des
  grammaires algébriques. Il s'écrit dans des fichiers <code>*.pp</code>.</p>
  <p>Une grammaire est constituée de lexèmes et de règles. La déclaration d'un
  lexème se fait de la manière suivante : <code>%token
  <em>namespace_in</em>:<em>name</em> <em>value</em> ->
  <em>namespace_out</em></code>, où <code><em>name</em></code> représente le nom
  du lexème, <code><em>value</em></code> représente sa valeur, au format
  <a href="http://pcre.org/">PCRE</a>, et <code><em>namespace_in</em></code> et
  <code><em>namespace_out</em></code> représentent les noms des espaces de noms
  et sont optionels (vaut <code>default</code> par défaut). Par exemple
  <code>number</code> qui représente un nombre composé de chiffres de
  <code>0</code> à <code>9</code> :</p>
  <pre><code language="pp">%token number \d+</code></pre>
  <p>Les espaces de noms représentent des sous-ensembles disjoints de lexèmes,
  utilisés pour faciliter les analyses. Une déclaration <code>%skip</code> est
  similaire à <code>%token</code> exceptée qu'elle représente un lexème à
  sauter, c'est à dire à ne pas considérer. Un exemple courant de lexèmes
  <code>%skip</code> est les espaces :</p>
  <pre><code language="pp">%skip space \s</code></pre>
  <p>Pour expliquer les règles, nous allons utiliser comme exemple la grammaire
  <code>Json.pp</code>, grammaire légèrement simplifiée du
  <a href="http://json.org/">langage JSON</a> (voir la
  <a href="https://tools.ietf.org/html/rfc4627">RFC4627</a>) :</p>
  <pre><code language="pp">%skip   space          \s
// Scalars.
%token  true           true
%token  false          false
%token  null           null
// Strings.
%token  quote_         "        -> string
%token  string:string  [^"]+
%token  string:_quote  "        -> default
// Objects.
%token  brace_         {
%token _brace          }
// Arrays.
%token  bracket_       \[
%token _bracket        \]
// Rest.
%token  colon          :
%token  comma          ,
%token  number         \d+

value:
    &amp;lt;true> | &amp;lt;false> | &amp;lt;null> | string() | object() | array() | number()

string:
    ::quote_:: &amp;lt;string> ::_quote::

number:
    &amp;lt;number>

#object:
    ::brace_:: pair() ( ::comma:: pair() )* ::_brace::

#pair:
    string() ::colon:: value()

#array:
    ::bracket_:: value() ( ::comma:: value() )* ::_bracket::</code></pre>
  <p>Nous remarquons que nous avons deux espaces de noms pour les lexèmes :
  <code>default</code> et <code>string</code> (cela permet de ne pas confondre
  les lexèmes <code>quote_</code> et <code>string:_quote</code> qui ont la même
  valeur). Nous remarquons ensuite la règle <code>value</code> qui est une
  disjonction de plusieurs lexèmes et règles. Les constructions du langage PP
  sont les suivantes :</p>
  <ul>
    <li><code><em>rule</em>()</code> pour appeler une règle ;</li>
    <li><code>&amp;lt;<em>token</em>></code> et <code>::<em>token</em>::</code>
    pour déclarer un lexème (les espaces de noms n'apparaissent pas ici) ;</li>
    <li><code>|</code> pour une disjonction (un choix) ;</li>
    <li><code>(<em>…</em>)</code> pour grouper ;</li>
    <li><code><em>e</em>?</code> pour dire que <code><em>e</em></code> est
    optionel ;</li>
    <li><code><em>e</em>+</code> pour dire que <code><em>e</em></code> peut
    apparaître 1 ou plusieurs fois ;</li>
    <li><code><em>e</em>*</code> pour dire que <code><em>e</em></code> peut
    apparaître 0 ou plusieurs fois ;</li>
    <li><code><em>e</em>{<em>x</em>,<em>y</em>}</code> pour dire que <em>e</em>
    peut apparaître entre <code><em>x</em></code> et <code><em>y</em></code>
    fois ;</li>
    <li><code>#<em>node</em></code> pour créer un nœud
    <code><em>node</em></code> dans l'arbre (détaillé plus tard) ;</li>
    <li><code><em>token</em>[<em>i</em>]</code> pour unifier des lexèmes entre
    eux (détaillé plus tard).</li>
  </ul>
  <p>Peu de constructions mais amplement suffisantes.</p>

  <h3 id="Processus_de_compilation" for="main-toc">Processus de compilation</h3>

  <p>Le processus de compilation qu'utilise <code>Hoa\Compiler\Llk</code> est
  classique. Il commence par analyser lexicalement la donnée textuelle, <ie /> à
  transformer notre donnée en une séquence de lexèmes. L'ordre de déclarations
  des lexèmes est primordial car l'analyseur lexical va les prendre les uns
  après les autres. Ensuite, c'est l'analyseur syntaxique qui entre en jeu. Il
  va compiler les règles écritent dans le langage PP en objet capable de
  supporter les caractéristiques LL(*) : retours en arrière, gestion des points
  de choix, gestion des erreurs etc.</p>
  <p>Si l'analyse lexicale est un succès, nous obtenons une trace. Cette trace
  peut être transformée en AST, pour <em lang="en">Abstract Syntax Tree</em>.
  Cet arbre représente notre donnée textuelle après analyses. Il a l'avantage de
  pouvoir être visiter (nous détaillerons plus loin), ce qui permet par exemple
  d'ajouter de nouvelles contraintes qui ne peuvent exprimer dans la grammaire,
  comme une vérification de type.</p>
  <p>Manipulons un peu <code>Hoa\Compiler\Llk</code>. Cette classe est un
  assistant pour lire une grammaire au format PP facilement. Elle prend en seule
  argument un flux en lecture vers la grammaire et retourne un compilateur
  <code>Hoa\Compiler\Llk\Parser</code> prêt à l'emploi. Sur ce compilateur, nous
  allons appeler la méthode <code>Hoa\Compiler\Llk\Parser::parse</code> pour
  analyser une donnée JSON. Si la donnée est correcte, nous aurons en retour un
  AST, sinon une exception sera levée. Enfin, nous allons utiliser le visiteur
  <code>Hoa\Compiler\Visitor\Dump</code> pour afficher notre AST :</p>
  <pre><code language="php">from('Hoa')
-> import('File.Read')
-> import('Compiler.Llk.~')
-> import('Compiler.Visitor.Dump');

// 1. Load grammar.
$compiler = Hoa\Compiler\Llk::load(new Hoa\File\Read('Json.pp'));

// 2. Parse a data.
$ast      = $compiler->parse('{"foo": true, "bar": [null, 42]}');

// 3. Dump the AST.
$dump     = new Hoa\Compiler\Visitor\Dump();
echo $dump->visit($ast);

/**
 * Will output:
 *     >  #object
 *     >  >  #pair
 *     >  >  >  token(string, foo)
 *     >  >  >  token(true, true)
 *     >  >  #pair
 *     >  >  >  token(string, bar)
 *     >  >  >  #array
 *     >  >  >  >  token(null, null)
 *     >  >  >  >  token(number, 42)
 */</code></pre>
  <p>Quand nous écrivons une grammaire, nous allons répéter ces trois tâches
  très régulièrement. C'est pourquoi, le script <code>hoa</code> (ou
  <code>hoa.bat</code>) propose la commande <code>pp</code>. Cette commande
  propose d'analyser une donnée par rapport à une donnée et d'appliquer un
  visiteur si besoin sur l'AST résultant. Notons que la lecture de la donnée peut
  se faire à travers un
  <a href="https://en.wikipedia.org/wiki/Pipeline_(Unix)"><em lang="en">pipe</em></a> :</p>
  <pre><code language="shell">$ echo '[1, [1, [2, 3], 5], 8]' | hoa pp Json.pp 0 --visitor dump
>  #array
>  >  token(number, 1)
>  >  #array
>  >  >  token(number, 1)
>  >  >  #array
>  >  >  >  token(number, 2)
>  >  >  >  token(number, 3)
>  >  >  token(number, 5)
>  >  token(number, 8)</code></pre>
  <p>C'est un moyen pratique pour tester rapidement des données par rapport à
  notre grammaire. Il ne faut pas hésiter à regarder l'aide de la commande
  <code>pp</code> pour plus de détails !</p>

  <h4 id="Erreurs" for="main-toc">Erreurs</h4>

  <p>Les erreurs de compilation sont remontées à travers des exceptions,
  ainsi :</p>
  <pre><code language="shell">$ echo '{"foo" true}' | hoa pp Json.pp 0 --visitor dump
Uncaught exception (Hoa\Compiler\Exception\UnexpectedToken):
Hoa\Compiler\Llk\Parser::parse(): (0) Unexpected token "true" (true) at line 1
and column 8:
{"foo" true}
       ↑
in hoa://Library/Compiler/Llk/Parser.php at line 1</code></pre>
  <p>Plusieurs exceptions peuvent remonter selon le contexte :</p>
  <ul>
    <li>durant l'analyse lexicale,
    <code>Hoa\Compiler\Exception\UnrecognizedToken</code> quand un lexème n'est
    pas reconnu, <ie /> quand la donnée textuelle ne peut plus être découpée en
    une séquence de lexèmes ;</li>
    <li>durant l'analyse syntaxique,
    <code>Hoa\Compiler\Exception\UnexpectedToken</code> quand un lexème
    n'est pas attendu, <ie /> qu'il ne permet plus de dériver les règles de la
    grammaire.</li>
  </ul>
  <p>L'exception parente est <code>Hoa\Compiler\Exception</code>.</p>

  <h4 id="Nœuds" for="main-toc">Nœuds</h4>

  <p>Le processus de compilation aboutit très souvent par la production d'un
  AST. Il est important de contrôler sa forme, sa taille, les données qui
  contient etc. C'est pourquoi il est nécessaire de comprendre la notation
  <code>#<em>node</em></code> car elle permet de créer des nœuds dans l'AST.
  Une précision tout d'abord, les lexèmes déclarés avec la syntaxe
  <code>&amp;lt;<em>token</em>></code> apparaîtront dans l'arbre, alors que les
  autres lexèmes, déclarés avec la syntaxe <code>::<em>token</em>::</code>, n'y
  apparaîtront pas. En effet, dans notre dernier exemple, les lexèmes
  <code>quote_</code>, <code>brace_</code>, <code>colon</code>,
  <code>comma</code> etc. n'apparaissent pas. Ensuite, il est important de noter
  que les déclarations de nœuds se surchargent les unes par rapport aux autres
  au sein d'une même règle. Par exemple, un nom de règle peut être précédé par
  <code>#</code>, comme pour la règle <code>#array</code>, ceci définit un nœud
  par défaut, mais il peut être surchargé. Par exemple, si nous écrivons :</p>
  <pre><code language="pp">#array:
    ::bracket_:: value() ( ::comma:: value() #bigarray )* ::_bracket::</code></pre>
  <p>Si le tableau ne contient qu'une seule valeur, le nœud s'appelera
  <code>#array</code>, sinon il s'appelera <code>#bigarray</code>, voyons
  plutôt :</p>
  <pre><code language="shell">$ echo '[42]' | hoa pp Json.pp 0 --visitor dump
  >  #array
  >  >  token(number, 42)
$ echo '[4, 2]' | hoa pp Json.pp 0 --visitor dump
>  #bigarray
>  >  token(number, 4)
>  >  token(number, 2)</code></pre>
  <p>Bien sûr, il peut arriver qu'un nœud soit crée ou pas selon le chemin
  emprunté dans la règle (selon la dérivation). Le mécanisme est normalement
  assez intuitif.</p>

  <h4 id="Unification" for="main-toc">Unification</h4>

  <p>Une caractéristique qu'apporte le langage PP par rapport à d'autres
  langages de grammaires connus est la capacité d'exprimer une unification de
  lexèmes. Imagineons la grammaire suivante :</p>
  <pre><code language="pp">%token  quote   '|"
%token  string  \w+

rule:
    ::quote:: &amp;lt;string> ::quote::</code></pre>
  <p>Les guillemets qui entourent la chaîne de caractère peuvent être de deux
  sortes : simple, avec le symbole « <code>'</code> », ou double, avec le
  symbole « <code>"</code> ». Ainsi, toutes les données <code>"foo"</code> et
  <code>'foo'</code> sont valides, mais également <code>"foo'</code> et
  <code>'foo"</code> !</p>
  <p>L'unification des lexèmes permet d'ajouter une contrainte
  supplémentaire sur la valeur des lexèmes à l'exécution. La syntaxe est la
  suivante : <code><em>token</em>[<em>i</em>]</code>. La valeur de
  <code><em>i</em></code> indique quels lexèmes vont devoir porter la même
  valeur. Et enfin, l'unification est local à une instance d'une règle,
  c'est à dire qu'il n'y a pas d'unifications entre les lexèmes de plusieurs
  règles et que cela s'applique sur la règle appelée uniquement. Ainsi,
  l'exemple devient :</p>
  <pre><code language="pp">rule:
    ::quote[0]:: &amp;lt;string> ::quote[0]::</code></pre>
  <p>L'unification trouve de nombreuses applications, comme par exemple unifier
  les noms des balises ouvrantes et fermantes du
  <a href="http://w3.org/TR/xml11/">langage XML</a>.</p>

  <!--
  hoa://Library/Compiler/Llk/Llk.pp
  vim plugins
  -->

  <h3 id="Traces" for="main-toc">Traces</h3>

  <h3 id="Abstract_Syntax_Tree" for="main-toc"><em lang="en">Abstract Syntax
  Tree</em></h3>

  <h2 id="Compilateur_de_compilateur_LL1" for="main-toc menu-toc">Compilateur de
  compilateur LL(1)</h2>

  <h2 id="Conclusion" for="main-toc menu-toc">Conclusion</h2>

</yield>
</definition>
